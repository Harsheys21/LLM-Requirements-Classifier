{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c3fdf9b",
   "metadata": {},
   "source": [
    "# Using LLMs for Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f082b69d",
   "metadata": {},
   "source": [
    "## Libraries Imported"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "3a40e2db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, pipeline\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b958aa0",
   "metadata": {},
   "source": [
    "## Extract from CSV File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "03233782",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./data/PURE_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "6e544e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=[\"Unnamed: 0\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d3cd2d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.rename(columns={'Requirement':'text', 'Name of Doc': 'source', 'Req/Not Req': 'label'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "b9d80998",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['y'] = np.where(df['label'] == 'Req', 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "0427d962",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>source</th>\n",
       "      <th>label</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>System Initialization performs those functions...</td>\n",
       "      <td>nasa x38.doc</td>\n",
       "      <td>Req</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Whenever a power-on reset occurs, System Initi...</td>\n",
       "      <td>nasa x38.doc</td>\n",
       "      <td>Req</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>As part of System Initialization , the Boot RO...</td>\n",
       "      <td>nasa x38.doc</td>\n",
       "      <td>Req</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>System Initialization shall [SRS014] initiate ...</td>\n",
       "      <td>nasa x38.doc</td>\n",
       "      <td>Req</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>System Initialization shall [SRS292] enable an...</td>\n",
       "      <td>nasa x38.doc</td>\n",
       "      <td>Req</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text        source label  y\n",
       "0  System Initialization performs those functions...  nasa x38.doc   Req  1\n",
       "1  Whenever a power-on reset occurs, System Initi...  nasa x38.doc   Req  1\n",
       "2  As part of System Initialization , the Boot RO...  nasa x38.doc   Req  1\n",
       "3  System Initialization shall [SRS014] initiate ...  nasa x38.doc   Req  1\n",
       "4  System Initialization shall [SRS292] enable an...  nasa x38.doc   Req  1"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "d82401c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_TEMPLATE = \"\"\"You are classifying software engineering sentences.\n",
    "\n",
    "Choose the best label:\n",
    "\n",
    "A) Req = the sentence describes a required system behavior, feature, or constraint \n",
    "   (e.g. uses \"shall\", \"must\", \"the system will\", or describes what the system or user can do).\n",
    "\n",
    "B) Not_Req = the sentence is background info, assumptions, document structure, or explanation.\n",
    "   It does NOT directly state a behavior or constraint the system must satisfy.\n",
    "\n",
    "Examples:\n",
    "Sentence: \"The interfaces must be made customizable or user-configurable to the extent possible.\"\n",
    "Answer: A\n",
    "\n",
    "Sentence: \"This document describes the main modules of the system.\"\n",
    "Answer: B\n",
    "\n",
    "Now classify this new sentence.\n",
    "\n",
    "Sentence: \"{sentence}\"\n",
    "Answer with exactly one letter: A or B.\n",
    "Answer:\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "0c4a0cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"google/flan-t5-large\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "0dbfcaa5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    model_name,\n",
    "    dtype=torch.float16,\n",
    "    low_cpu_mem_usage=True,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "classifier = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "17a02dda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results:\n",
      "Accuracy: 0.7861799217731421\n",
      "Precision: 0.899343544857768\n",
      "Recall: 0.776937618147448\n",
      "F1 Score: 0.8336713995943205\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "model.eval()\n",
    "\n",
    "texts = df[\"text\"].tolist()\n",
    "labels = df[\"label\"].tolist()\n",
    "prompts = [PROMPT_TEMPLATE.format(sentence=x) for x in texts]\n",
    "\n",
    "preds = []\n",
    "truth = []\n",
    "\n",
    "with torch.inference_mode():\n",
    "    outputs = classifier(\n",
    "        prompts,\n",
    "        max_new_tokens=2,\n",
    "        num_beams=3,\n",
    "        do_sample=False,\n",
    "        batch_size=16,\n",
    "        truncation=True\n",
    "    )\n",
    "\n",
    "for out, y in zip(outputs, labels):\n",
    "    raw = out[\"generated_text\"]\n",
    "    first = raw.strip().split()[0].upper()\n",
    "    \n",
    "    if first.startswith(\"A\"):\n",
    "        pred = \"Req\"\n",
    "    elif first.startswith(\"B\"):\n",
    "        pred = \"Not_Req\"\n",
    "    else:\n",
    "        pred = \"Not_Req\" \n",
    "\n",
    "    preds.append(pred)\n",
    "    # Normalize truth\n",
    "    true = \"Req\" if str(y).lower().startswith(\"req\") else \"Not_Req\"\n",
    "    truth.append(true)\n",
    "\n",
    "# Metrics\n",
    "accuracy = accuracy_score(truth, preds)\n",
    "precision = precision_score([t==\"Req\" for t in truth], [p==\"Req\" for p in preds])\n",
    "recall = recall_score([t==\"Req\" for t in truth], [p==\"Req\" for p in preds])\n",
    "f1 = f1_score([t==\"Req\" for t in truth], [p==\"Req\" for p in preds])\n",
    "\n",
    "print(\"\\nResults:\") \n",
    "print(\"Accuracy:\", accuracy) \n",
    "print(\"Precision:\", precision) \n",
    "print(\"Recall:\", recall) \n",
    "print(\"F1 Score:\", f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "ab970d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_EXAMPLES = \"\"\"Examples:\n",
    "Sentence: \"The interfaces must be made customizable or user-configurable to the extent possible.\"\n",
    "Answer: A\n",
    "\n",
    "Sentence: \"This document describes the main modules of the system.\"\n",
    "Answer: B\n",
    "\"\"\"\n",
    "\n",
    "def build_meta_prompt(req, notreq):\n",
    "    hard_examples_text = \"\"\n",
    "\n",
    "    # misclassified true-Req → A\n",
    "    for sent, _, _ in req[:3]:\n",
    "        hard_examples_text += f'Sentence: \"{sent}\"; Answer: A\\n'\n",
    "\n",
    "    # misclassified true-Not_Req → B\n",
    "    for sent, _, _ in notreq[:3]:\n",
    "        hard_examples_text += f'Sentence: \"{sent}\"; Answer: B\\n'\n",
    "\n",
    "    prompt = f\"\"\"You are classifying software engineering sentences.\n",
    "\n",
    "Choose the best label:\n",
    "\n",
    "A) Req = the sentence describes a required system behavior, feature, or constraint \n",
    "   (e.g. uses \"shall\", \"must\", \"the system will\", or describes what the system or user can do).\n",
    "\n",
    "B) Not_Req = the sentence is background info, assumptions, document structure, or explanation.\n",
    "   It does NOT directly state a behavior or constraint the system must satisfy.\n",
    "\n",
    "{BASE_EXAMPLES}\n",
    "\n",
    "Examples the model got wrong previously:\n",
    "{hard_examples_text}\n",
    "\n",
    "Now classify this new sentence.\n",
    "\n",
    "Sentence: \"{{sentence}}\"\n",
    "Answer with exactly one letter: A or B.\n",
    "Answer:\n",
    "\"\"\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "0b6baeae-bc34-4108-a65c-e474983558fd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5 misclassified req examples to improve the prompt.\n",
      "Found 0 misclassified not req examples to improve the prompt.\n",
      "\n",
      "Results:\n",
      "Accuracy: 0.7907431551499348\n",
      "Precision: 0.8992416034669556\n",
      "Recall: 0.7844990548204159\n",
      "F1 Score: 0.8379606259464917\n",
      "Found 5 misclassified req examples to improve the prompt.\n",
      "Found 0 misclassified not req examples to improve the prompt.\n",
      "\n",
      "Results:\n",
      "Accuracy: 0.7900912646675359\n",
      "Precision: 0.9\n",
      "Recall: 0.782608695652174\n",
      "F1 Score: 0.8372093023255814\n"
     ]
    }
   ],
   "source": [
    "prev_f1 = 0\n",
    "improvement_threshold = 0.001   # stop if improvement < 0.1%\n",
    "\n",
    "while True:\n",
    "    # 1. run inference\n",
    "    # 2. compute f1 score → f1\n",
    "\n",
    "    if f1 - prev_f1 < improvement_threshold:\n",
    "        break\n",
    "\n",
    "    prev_f1 = f1\n",
    "\n",
    "    req_inc = []\n",
    "    notreq_inc = []\n",
    "    \n",
    "    for sent, true, pred in zip(df[\"text\"], truth, preds):\n",
    "        if true != pred and true == 'Req':\n",
    "            req_inc.append((sent, true, pred))\n",
    "        elif true != pred and true == 'Not_Req':\n",
    "            notreq_inc.append((sent, true, pred))\n",
    "    \n",
    "        if len(req_inc) >= 5 or len(notreq_inc) >= 5:\n",
    "            break\n",
    "    \n",
    "    \n",
    "    print(f\"Found {len(req_inc)} misclassified req examples to improve the prompt.\")\n",
    "    print(f\"Found {len(notreq_inc)} misclassified not req examples to improve the prompt.\")\n",
    "\n",
    "\n",
    "    meta_prompt = build_meta_prompt(req_inc, notreq_inc)\n",
    "\n",
    "    import torch\n",
    "    from tqdm import tqdm\n",
    "    from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    texts = df[\"text\"].tolist()\n",
    "    labels = df[\"label\"].tolist()\n",
    "    prompts = [meta_prompt.format(sentence=x) for x in texts]\n",
    "    \n",
    "    preds = []\n",
    "    truth = []\n",
    "    \n",
    "    with torch.inference_mode():\n",
    "        outputs = classifier(\n",
    "            prompts,\n",
    "            max_new_tokens=2,\n",
    "            num_beams=3,\n",
    "            do_sample=False,\n",
    "            batch_size=16,\n",
    "            truncation=True\n",
    "        )\n",
    "    \n",
    "    for out, y in zip(outputs, labels):\n",
    "        raw = out[\"generated_text\"]\n",
    "        first = raw.strip().split()[0].upper()\n",
    "        \n",
    "        if first.startswith(\"A\"):\n",
    "            pred = \"Req\"\n",
    "        elif first.startswith(\"B\"):\n",
    "            pred = \"Not_Req\"\n",
    "        else:\n",
    "            pred = \"Not_Req\" \n",
    "            \n",
    "        preds.append(pred)\n",
    "        # Normalize truth\n",
    "        true = \"Req\" if str(y).lower().startswith(\"req\") else \"Not_Req\"\n",
    "        truth.append(true)\n",
    "    \n",
    "    # Metrics\n",
    "    accuracy = accuracy_score(truth, preds)\n",
    "    precision = precision_score([t==\"Req\" for t in truth], [p==\"Req\" for p in preds])\n",
    "    recall = recall_score([t==\"Req\" for t in truth], [p==\"Req\" for p in preds])\n",
    "    f1 = f1_score([t==\"Req\" for t in truth], [p==\"Req\" for p in preds])\n",
    "    \n",
    "    print(\"\\nResults:\") \n",
    "    print(\"Accuracy:\", accuracy) \n",
    "    print(\"Precision:\", precision) \n",
    "    print(\"Recall:\", recall) \n",
    "    print(\"F1 Score:\", f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d6a42f4-d801-4c36-a6f9-83343e84dc0d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM Req Classifier",
   "language": "python",
   "name": "llm-req"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
