{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c3fdf9b",
   "metadata": {},
   "source": [
    "# Using LLMs for Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f082b69d",
   "metadata": {},
   "source": [
    "## Libraries Imported"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "3a40e2db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, pipeline\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b958aa0",
   "metadata": {},
   "source": [
    "## Extract from CSV File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "03233782",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./data/PURE_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "6e544e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=[\"Unnamed: 0\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "d3cd2d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.rename(columns={'Requirement':'text', 'Name of Doc': 'source', 'Req/Not Req': 'label'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "b9d80998",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['y'] = np.where(df['label'] == 'Req', 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "0427d962",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>source</th>\n",
       "      <th>label</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>System Initialization performs those functions...</td>\n",
       "      <td>nasa x38.doc</td>\n",
       "      <td>Req</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Whenever a power-on reset occurs, System Initi...</td>\n",
       "      <td>nasa x38.doc</td>\n",
       "      <td>Req</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>As part of System Initialization , the Boot RO...</td>\n",
       "      <td>nasa x38.doc</td>\n",
       "      <td>Req</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>System Initialization shall [SRS014] initiate ...</td>\n",
       "      <td>nasa x38.doc</td>\n",
       "      <td>Req</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>System Initialization shall [SRS292] enable an...</td>\n",
       "      <td>nasa x38.doc</td>\n",
       "      <td>Req</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text        source label  y\n",
       "0  System Initialization performs those functions...  nasa x38.doc   Req  1\n",
       "1  Whenever a power-on reset occurs, System Initi...  nasa x38.doc   Req  1\n",
       "2  As part of System Initialization , the Boot RO...  nasa x38.doc   Req  1\n",
       "3  System Initialization shall [SRS014] initiate ...  nasa x38.doc   Req  1\n",
       "4  System Initialization shall [SRS292] enable an...  nasa x38.doc   Req  1"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "d82401c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_TEMPLATE = \"\"\"You are classifying software engineering sentences.\n",
    "\n",
    "Choose the best label:\n",
    "\n",
    "A) Req = the sentence describes a required system behavior, feature, or constraint \n",
    "   (e.g. uses \"shall\", \"must\", \"the system will\", or describes what the system or user can do).\n",
    "\n",
    "B) Not_Req = the sentence is background info, assumptions, document structure, or explanation.\n",
    "   It does NOT directly state a behavior or constraint the system must satisfy.\n",
    "\n",
    "Examples:\n",
    "Sentence: \"The interfaces must be made customizable or user-configurable to the extent possible.\"\n",
    "Answer: A\n",
    "\n",
    "Sentence: \"This document describes the main modules of the system.\"\n",
    "Answer: B\n",
    "\n",
    "Now classify this new sentence.\n",
    "\n",
    "Sentence: \"{sentence}\"\n",
    "Answer with exactly one letter: A or B.\n",
    "Answer:\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "0c91c196-b37e-4766-a437-b56924f65e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "INITIAL_PROMPT = \"\"\"\n",
    "Classify this sentence as Req or Not_Req. \n",
    "Requirement = a required system behavior, feature, or constraint (e.g. sentences with \"shall\", \"must\", \"the system will\", or describing what the system or user can do). \n",
    "Not_Req = background, document structure, assumptions, or explanations that are not required behavior. \n",
    "\n",
    "Examples: \n",
    "Sentence: \"The interfaces must be made customizable or user-configurable to the extent possible.\" \n",
    "Label: Req \n",
    "\n",
    "Sentence: \"This document describes the main modules of the system.\" \n",
    "Label: Not_Req \n",
    "\n",
    "Sentence: \"{sentence}\" \n",
    "Answer with exactly: Req or Not_Req. \n",
    "Label: \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "0c4a0cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"google/flan-t5-large\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "0dbfcaa5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    model_name,\n",
    "    dtype=torch.float16,\n",
    "    low_cpu_mem_usage=True,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "classifier = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "bbc5c1b2-7f80-49c0-a87e-af4067345d51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results:\n",
      "Accuracy: 0.7229465449804433\n",
      "Precision: 0.9126466753585397\n",
      "Recall: 0.6616257088846881\n",
      "F1 Score: 0.7671232876712328\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "texts = df[\"text\"].tolist()\n",
    "labels = df[\"label\"].tolist()\n",
    "prompts = [INITIAL_PROMPT.format(sentence=x) for x in texts]\n",
    "\n",
    "preds = []\n",
    "truth = []\n",
    "\n",
    "with torch.inference_mode():\n",
    "    outputs = classifier(\n",
    "        prompts,\n",
    "        max_new_tokens=2,\n",
    "        num_beams=1,\n",
    "        do_sample=False,\n",
    "        batch_size=16,\n",
    "        truncation=True\n",
    "    )\n",
    "\n",
    "for out, y in zip(outputs, labels):\n",
    "    raw = out[\"generated_text\"]\n",
    "    raw_label = raw.strip().split()[0].strip().lower()\n",
    "    \n",
    "    if raw_label in {\"req\", \"requirement\"}:\n",
    "        pred = \"Req\"\n",
    "    elif raw_label in {\"not_req\", \"not-req\", \"notreq\", \"non-requirement\", \"nonrequirement\"}:\n",
    "        pred = \"Not_Req\"\n",
    "    else:\n",
    "        pred = \"Not_Req\"\n",
    "\n",
    "    preds.append(pred)\n",
    "    true = \"Req\" if str(y).lower().startswith(\"req\") else \"Not_Req\"\n",
    "    truth.append(true)\n",
    "\n",
    "# Metrics\n",
    "accuracy = accuracy_score(truth, preds)\n",
    "precision = precision_score([t==\"Req\" for t in truth], [p==\"Req\" for p in preds])\n",
    "recall = recall_score([t==\"Req\" for t in truth], [p==\"Req\" for p in preds])\n",
    "f1 = f1_score([t==\"Req\" for t in truth], [p==\"Req\" for p in preds])\n",
    "\n",
    "print(\"\\nResults:\") \n",
    "print(\"Accuracy:\", accuracy) \n",
    "print(\"Precision:\", precision) \n",
    "print(\"Recall:\", recall) \n",
    "print(\"F1 Score:\", f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "17a02dda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results:\n",
      "Accuracy: 0.7861799217731421\n",
      "Precision: 0.899343544857768\n",
      "Recall: 0.776937618147448\n",
      "F1 Score: 0.8336713995943205\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "texts = df[\"text\"].tolist()\n",
    "labels = df[\"label\"].tolist()\n",
    "prompts = [PROMPT_TEMPLATE.format(sentence=x) for x in texts]\n",
    "\n",
    "preds = []\n",
    "truth = []\n",
    "\n",
    "with torch.inference_mode():\n",
    "    outputs = classifier(\n",
    "        prompts,\n",
    "        max_new_tokens=2,\n",
    "        num_beams=1,\n",
    "        do_sample=False,\n",
    "        batch_size=16,\n",
    "        truncation=True\n",
    "    )\n",
    "\n",
    "for out, y in zip(outputs, labels):\n",
    "    raw = out[\"generated_text\"]\n",
    "    first = raw.strip().split()[0].upper()\n",
    "    \n",
    "    if first.startswith(\"A\"):\n",
    "        pred = \"Req\"\n",
    "    elif first.startswith(\"B\"):\n",
    "        pred = \"Not_Req\"\n",
    "    else:\n",
    "        pred = \"Not_Req\" \n",
    "\n",
    "    preds.append(pred)\n",
    "    true = \"Req\" if str(y).lower().startswith(\"req\") else \"Not_Req\"\n",
    "    truth.append(true)\n",
    "\n",
    "# Metrics\n",
    "accuracy = accuracy_score(truth, preds)\n",
    "precision = precision_score([t==\"Req\" for t in truth], [p==\"Req\" for p in preds])\n",
    "recall = recall_score([t==\"Req\" for t in truth], [p==\"Req\" for p in preds])\n",
    "f1 = f1_score([t==\"Req\" for t in truth], [p==\"Req\" for p in preds])\n",
    "\n",
    "print(\"\\nResults:\") \n",
    "print(\"Accuracy:\", accuracy) \n",
    "print(\"Precision:\", precision) \n",
    "print(\"Recall:\", recall) \n",
    "print(\"F1 Score:\", f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "ab970d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_EXAMPLES = \"\"\"Examples:\n",
    "Sentence: \"The interfaces must be made customizable or user-configurable to the extent possible.\"\n",
    "Answer: A\n",
    "\n",
    "Sentence: \"This document describes the main modules of the system.\"\n",
    "Answer: B\n",
    "\"\"\"\n",
    "\n",
    "def build_meta_prompt(req, notreq):\n",
    "    hard_examples_text = \"\"\n",
    "\n",
    "    for sent, _, _ in req[:3]:\n",
    "        hard_examples_text += f'Sentence: \"{sent}\"; Answer: A\\n'\n",
    "\n",
    "    for sent, _, _ in notreq[:3]:\n",
    "        hard_examples_text += f'Sentence: \"{sent}\"; Answer: B\\n'\n",
    "\n",
    "    prompt = f\"\"\"You are classifying software engineering sentences.\n",
    "\n",
    "Choose the best label:\n",
    "\n",
    "A) Req = the sentence describes a required system behavior, feature, or constraint \n",
    "   (e.g. uses \"shall\", \"must\", \"the system will\", or describes what the system or user can do).\n",
    "\n",
    "B) Not_Req = the sentence is background info, assumptions, document structure, or explanation.\n",
    "   It does NOT directly state a behavior or constraint the system must satisfy.\n",
    "\n",
    "{BASE_EXAMPLES}\n",
    "\n",
    "Examples the model got wrong previously:\n",
    "{hard_examples_text}\n",
    "\n",
    "Now classify this new sentence.\n",
    "\n",
    "Sentence: \"{{sentence}}\"\n",
    "Answer with exactly one letter: A or B.\n",
    "Answer:\n",
    "\"\"\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "0b6baeae-bc34-4108-a65c-e474983558fd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5 misclassified req examples to improve the prompt.\n",
      "Found 0 misclassified not req examples to improve the prompt.\n",
      "\n",
      "Results:\n",
      "Accuracy: 0.7907431551499348\n",
      "Precision: 0.8992416034669556\n",
      "Recall: 0.7844990548204159\n",
      "F1 Score: 0.8379606259464917\n",
      "Found 5 misclassified req examples to improve the prompt.\n",
      "Found 0 misclassified not req examples to improve the prompt.\n",
      "\n",
      "Results:\n",
      "Accuracy: 0.7900912646675359\n",
      "Precision: 0.9\n",
      "Recall: 0.782608695652174\n",
      "F1 Score: 0.8372093023255814\n"
     ]
    }
   ],
   "source": [
    "prev_f1 = 0\n",
    "improvement_threshold = 0.001  \n",
    "\n",
    "while True:\n",
    "\n",
    "    if f1 - prev_f1 < improvement_threshold:\n",
    "        break\n",
    "\n",
    "    prev_f1 = f1\n",
    "\n",
    "    req_inc = []\n",
    "    notreq_inc = []\n",
    "    \n",
    "    for sent, true, pred in zip(df[\"text\"], truth, preds):\n",
    "        if true != pred and true == 'Req':\n",
    "            req_inc.append((sent, true, pred))\n",
    "        elif true != pred and true == 'Not_Req':\n",
    "            notreq_inc.append((sent, true, pred))\n",
    "    \n",
    "        if len(req_inc) >= 5 or len(notreq_inc) >= 5:\n",
    "            break\n",
    "    \n",
    "    \n",
    "    print(f\"Found {len(req_inc)} misclassified req examples to improve the prompt.\")\n",
    "    print(f\"Found {len(notreq_inc)} misclassified not req examples to improve the prompt.\")\n",
    "\n",
    "\n",
    "    meta_prompt = build_meta_prompt(req_inc, notreq_inc)\n",
    "    model.eval()\n",
    "    \n",
    "    texts = df[\"text\"].tolist()\n",
    "    labels = df[\"label\"].tolist()\n",
    "    prompts = [meta_prompt.format(sentence=x) for x in texts]\n",
    "    \n",
    "    preds = []\n",
    "    truth = []\n",
    "    \n",
    "    with torch.inference_mode():\n",
    "        outputs = classifier(\n",
    "            prompts,\n",
    "            max_new_tokens=2,\n",
    "            num_beams=3,\n",
    "            do_sample=False,\n",
    "            batch_size=16,\n",
    "            truncation=True\n",
    "        )\n",
    "    \n",
    "    for out, y in zip(outputs, labels):\n",
    "        raw = out[\"generated_text\"]\n",
    "        first = raw.strip().split()[0].upper()\n",
    "        \n",
    "        if first.startswith(\"A\"):\n",
    "            pred = \"Req\"\n",
    "        elif first.startswith(\"B\"):\n",
    "            pred = \"Not_Req\"\n",
    "        else:\n",
    "            pred = \"Not_Req\" \n",
    "            \n",
    "        preds.append(pred)\n",
    "        # Normalize truth\n",
    "        true = \"Req\" if str(y).lower().startswith(\"req\") else \"Not_Req\"\n",
    "        truth.append(true)\n",
    "    \n",
    "    # Metrics\n",
    "    accuracy = accuracy_score(truth, preds)\n",
    "    precision = precision_score([t==\"Req\" for t in truth], [p==\"Req\" for p in preds])\n",
    "    recall = recall_score([t==\"Req\" for t in truth], [p==\"Req\" for p in preds])\n",
    "    f1 = f1_score([t==\"Req\" for t in truth], [p==\"Req\" for p in preds])\n",
    "    \n",
    "    print(\"\\nResults:\") \n",
    "    print(\"Accuracy:\", accuracy) \n",
    "    print(\"Precision:\", precision) \n",
    "    print(\"Recall:\", recall) \n",
    "    print(\"F1 Score:\", f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "8d6a42f4-d801-4c36-a6f9-83343e84dc0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temperature Experiment:\n",
      "\n",
      "Temp:0.1\n",
      "Accuracy:  0.786\n",
      "Precision: 0.899\n",
      "Recall:    0.777\n",
      "F1 Score:  0.834\n",
      "Temp:0.3\n",
      "Accuracy:  0.786\n",
      "Precision: 0.899\n",
      "Recall:    0.777\n",
      "F1 Score:  0.834\n",
      "Temp:0.7\n",
      "Accuracy:  0.786\n",
      "Precision: 0.899\n",
      "Recall:    0.777\n",
      "F1 Score:  0.834\n",
      "Temp:1.0\n",
      "Accuracy:  0.786\n",
      "Precision: 0.899\n",
      "Recall:    0.777\n",
      "F1 Score:  0.834\n"
     ]
    }
   ],
   "source": [
    "texts = df[\"text\"].tolist()\n",
    "labels = df[\"label\"].tolist()\n",
    "prompts = [PROMPT_TEMPLATE.format(sentence=x) for x in texts]\n",
    "\n",
    "temperatures = [0.1, 0.3, 0.7, 1.0]\n",
    "\n",
    "print(\"Temperature Experiment:\\n\")\n",
    "\n",
    "for temp in temperatures:\n",
    "    print(f\"Temp:{temp}\")\n",
    "\n",
    "    preds = []\n",
    "    truth = []\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        outputs = classifier(\n",
    "            prompts,\n",
    "            batch_size=16,\n",
    "            truncation=True,\n",
    "            do_sample=True,\n",
    "            temperature=temp,\n",
    "            max_new_tokens=2,\n",
    "        )\n",
    "\n",
    "    for out, y in zip(outputs, labels):\n",
    "        raw = out[\"generated_text\"]\n",
    "        first = raw.strip().split()[0].upper()\n",
    "\n",
    "        if first.startswith(\"A\"):\n",
    "            pred = \"Req\"\n",
    "        else:\n",
    "            pred = \"Not_Req\"\n",
    "\n",
    "        preds.append(pred)\n",
    "        truth.append(\"Req\" if str(y).lower().startswith(\"req\") else \"Not_Req\")\n",
    "\n",
    "    # Metrics\n",
    "    acc = accuracy_score(truth, preds)\n",
    "    prec = precision_score([t==\"Req\" for t in truth], [p==\"Req\" for p in preds])\n",
    "    rec = recall_score([t==\"Req\" for t in truth], [p==\"Req\" for p in preds])\n",
    "    f1 = f1_score([t==\"Req\" for t in truth], [p==\"Req\" for p in preds])\n",
    "\n",
    "    print(f\"Accuracy:  {acc:.3f}\")\n",
    "    print(f\"Precision: {prec:.3f}\")\n",
    "    print(f\"Recall:    {rec:.3f}\")\n",
    "    print(f\"F1 Score:  {f1:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "b2c2a2b9-a022-49fa-9691-567e4e31a640",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: google/flan-t5-small\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.696\n",
      "Precision: 0.694\n",
      "Recall:    1.000\n",
      "F1 Score:  0.820\n",
      "Model: google/flan-t5-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.720\n",
      "Precision: 0.731\n",
      "Recall:    0.940\n",
      "F1 Score:  0.823\n",
      "Model: google/flan-t5-large\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.786\n",
      "Precision: 0.899\n",
      "Recall:    0.777\n",
      "F1 Score:  0.834\n"
     ]
    }
   ],
   "source": [
    "model_names = [\n",
    "    \"google/flan-t5-small\",\n",
    "    \"google/flan-t5-base\",\n",
    "    \"google/flan-t5-large\",\n",
    "]\n",
    "\n",
    "texts = df[\"text\"].tolist()\n",
    "labels = df[\"label\"].tolist()\n",
    "prompts = [PROMPT_TEMPLATE.format(sentence=x) for x in texts]\n",
    "\n",
    "results = []\n",
    "\n",
    "for model_name in model_names:\n",
    "    print(f\"Model: {model_name}\")\n",
    "\n",
    "    # Load model + tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "        model_name,\n",
    "        dtype=torch.float16,\n",
    "        low_cpu_mem_usage=True,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "    classifier = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "    preds = []\n",
    "    truth = []\n",
    "\n",
    "    model.eval()\n",
    "    with torch.inference_mode():\n",
    "        outputs = classifier(\n",
    "            prompts,\n",
    "            batch_size=16,\n",
    "            truncation=True,\n",
    "            max_new_tokens=2,\n",
    "            num_beams=1,     # deterministic greedy decoding\n",
    "            do_sample=False,\n",
    "        )\n",
    "\n",
    "    for out, y in zip(outputs, labels):\n",
    "        raw = out[\"generated_text\"]\n",
    "        first = raw.strip().split()[0].upper()\n",
    "\n",
    "        if first.startswith(\"A\"):\n",
    "            pred = \"Req\"\n",
    "        else:\n",
    "            pred = \"Not_Req\"\n",
    "\n",
    "        preds.append(pred)\n",
    "        truth.append(\"Req\" if str(y).lower().startswith(\"req\") else \"Not_Req\")\n",
    "\n",
    "    # Metrics\n",
    "    acc = accuracy_score(truth, preds)\n",
    "    prec = precision_score([t == \"Req\" for t in truth], [p == \"Req\" for p in preds])\n",
    "    rec = recall_score([t == \"Req\" for t in truth], [p == \"Req\" for p in preds])\n",
    "    f1 = f1_score([t == \"Req\" for t in truth], [p == \"Req\" for p in preds])\n",
    "\n",
    "    print(f\"Accuracy:  {acc:.3f}\")\n",
    "    print(f\"Precision: {prec:.3f}\")\n",
    "    print(f\"Recall:    {rec:.3f}\")\n",
    "    print(f\"F1 Score:  {f1:.3f}\")\n",
    "\n",
    "    results.append({\n",
    "        \"model\": model_name,\n",
    "        \"accuracy\": acc,\n",
    "        \"precision\": prec,\n",
    "        \"recall\": rec,\n",
    "        \"f1\": f1,\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "3771a1bd-af91-481c-9df4-b5645495bf1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results:\n",
      "google/flan-t5-small: Acc=0.696, Prec=0.694, Rec=1.000, F1=0.820\n",
      "google/flan-t5-base: Acc=0.720, Prec=0.731, Rec=0.940, F1=0.823\n",
      "google/flan-t5-large: Acc=0.786, Prec=0.899, Rec=0.777, F1=0.834\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nResults:\")\n",
    "for r in results:\n",
    "    print(\n",
    "        f\"{r['model']}: \"\n",
    "        f\"Acc={r['accuracy']:.3f}, \"\n",
    "        f\"Prec={r['precision']:.3f}, \"\n",
    "        f\"Rec={r['recall']:.3f}, \"\n",
    "        f\"F1={r['f1']:.3f}\"\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM Req Classifier",
   "language": "python",
   "name": "llm-req"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
